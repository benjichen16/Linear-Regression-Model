{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Inits data module with dataset url and column names.\n",
    "        \"\"\"\n",
    "        self.url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "        self.column_names = [\n",
    "            \"mpg\",\n",
    "            \"cylinders\",\n",
    "            \"displacement\",\n",
    "            \"horsepower\",\n",
    "            \"weight\",\n",
    "            \"acceleration\",\n",
    "            \"model_year\",\n",
    "            \"origin\",\n",
    "        ]\n",
    "    def returnData(self):\n",
    "        return self.dataset\n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Load the dataset and drop NaN rows.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(\n",
    "            self.url,\n",
    "            names=self.column_names,\n",
    "            na_values=\"?\",\n",
    "            comment=\"\\t\",\n",
    "            sep=\" \",\n",
    "            skipinitialspace=True,\n",
    "        ).dropna()\n",
    "\n",
    "    def bin_feature(self, feature: str, bins: pd.IntervalIndex) -> None:\n",
    "        \"\"\"\n",
    "        Perform binning operation on the column named \"feature\" in the given\n",
    "        dataframe, and encode the binned feature into one-hot vectors.\n",
    "\n",
    "        Args:\n",
    "            feature: Name of the feature to bin.\n",
    "            bins: Bin intervals represented by pd.IntervalIndex.\n",
    "        \"\"\"\n",
    "        self.dataset[feature] = pd.cut(self.dataset[feature], bins)\n",
    "        self.dataset = pd.get_dummies(self.dataset, columns = [feature])\n",
    "\n",
    "    def one_hot_encode(self, features: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Encode a list of features in a dataframe as one-hot vectors and drop the\n",
    "        (original) unencoded feature columns.\n",
    "\n",
    "        Args:\n",
    "            features: The column names of the features that need to be encoded.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.get_dummies(self.dataset, columns = features)\n",
    "\n",
    "    def cross_feature(self, feature_a: str, feature_b: str) -> None:\n",
    "        \"\"\"\n",
    "        Make a new crossed feature by multiplying feature_a and feature_b, and\n",
    "        name the new feature as \"crossed_feature\".\n",
    "\n",
    "        Args:\n",
    "            feature_a: The column name of feature A.\n",
    "            feature_b: The column name of feature B.\n",
    "        \"\"\"\n",
    "        self.dataset[\"crossed_feature\"] = self.dataset[feature_a] * self.dataset[feature_b]\n",
    "        self.dataset = self.dataset.drop(columns = [feature_a, feature_b])\n",
    "\n",
    "    def normalize(self) -> None:\n",
    "        \"\"\"\n",
    "        Use min-max normalizaton to normalize the dataset. The equation is provided below.\n",
    "\n",
    "        d_normalized = (d - min(d)) / (max(d) - min(d))\n",
    "        \"\"\"\n",
    "        for i in self.dataset.columns:\n",
    "            column_max = self.dataset[i].max()\n",
    "            column_min = self.dataset[i].min()\n",
    "            self.dataset[i] = (self.dataset[i] - column_min) / (column_max - column_min)\n",
    "        \n",
    "\n",
    "    def train_val_test_split(\n",
    "        self, val_size: float = 0.2, test_size: float = 0.5, seed: int = 144\n",
    "    ) -> Tuple[Tuple[np.ndarray], Tuple[np.ndarray], Tuple[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Split a dataframe into features and labels for train, validation, and test sets.\n",
    "\n",
    "        Args:\n",
    "            val_size: The proportion of the dataset to include in the validation set.\n",
    "            It must be a float type.\n",
    "\n",
    "            test_size: The proportion of the dataset to include in the validation set. It\n",
    "            must be a float type.\n",
    "\n",
    "            seed: Controls the shuffling of the dataframe. Do not modify the default seed.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing train, validation, and test features and labels.\n",
    "        \"\"\"\n",
    "        # ========== YOUR CODE STARTS HERE ==========\n",
    "        train_split,test_split = train_test_split(self.dataset, test_size = test_size, random_state = seed, shuffle=True)\n",
    "        train_split,val_split = train_test_split(train_split, test_size = val_size, random_state = seed, shuffle=True)\n",
    "\n",
    "        x_test = test_split.iloc[:,1:]\n",
    "        y_test = test_split.iloc[:, 0]\n",
    "\n",
    "        x_train = train_split.iloc[:, 1:]\n",
    "        y_train = train_split.iloc[:,0]\n",
    "\n",
    "        x_val = val_split.iloc[:, 1:]\n",
    "        y_val = val_split.iloc[:, 0]\n",
    "\n",
    "\n",
    "        # ========== YOUR CODE ENDS HERE ==========\n",
    "\n",
    "        return (\n",
    "            (x_train, y_train),\n",
    "            (x_val, y_val),\n",
    "            (x_test, y_test),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((     horsepower    weight  model_year_(69, 74]  model_year_(74, 79]  \\\n",
       "  274    0.309783  0.345052                  0.0                  1.0   \n",
       "  146    0.157609  0.145166                  1.0                  0.0   \n",
       "  130    0.184783  0.237596                  1.0                  0.0   \n",
       "  341    0.347826  0.315282                  0.0                  0.0   \n",
       "  80     0.217391  0.221718                  1.0                  0.0   \n",
       "  ..          ...       ...                  ...                  ...   \n",
       "  213    0.538043  0.692373                  0.0                  1.0   \n",
       "  53     0.103261  0.045364                  1.0                  0.0   \n",
       "  187    0.510870  0.737737                  0.0                  1.0   \n",
       "  98     0.293478  0.472073                  1.0                  0.0   \n",
       "  16     0.277174  0.329175                  1.0                  0.0   \n",
       "  \n",
       "       model_year_(79, 84]  cylinders_3  cylinders_4  cylinders_5  cylinders_6  \\\n",
       "  274                  0.0          0.0          0.0          1.0          0.0   \n",
       "  146                  0.0          0.0          1.0          0.0          0.0   \n",
       "  130                  0.0          0.0          1.0          0.0          0.0   \n",
       "  341                  1.0          0.0          0.0          0.0          1.0   \n",
       "  80                   0.0          0.0          1.0          0.0          0.0   \n",
       "  ..                   ...          ...          ...          ...          ...   \n",
       "  213                  0.0          0.0          0.0          0.0          0.0   \n",
       "  53                   0.0          0.0          1.0          0.0          0.0   \n",
       "  187                  0.0          0.0          0.0          0.0          0.0   \n",
       "  98                   0.0          0.0          0.0          0.0          1.0   \n",
       "  16                   0.0          0.0          0.0          0.0          1.0   \n",
       "  \n",
       "       cylinders_8  origin_1  origin_2  origin_3  crossed_feature  \n",
       "  274          0.0       0.0       1.0       0.0         0.209160  \n",
       "  146          0.0       1.0       0.0       0.0         0.074459  \n",
       "  130          0.0       1.0       0.0       0.0         0.197056  \n",
       "  341          0.0       1.0       0.0       0.0         0.225939  \n",
       "  80           0.0       1.0       0.0       0.0         0.186494  \n",
       "  ..           ...       ...       ...       ...              ...  \n",
       "  213          1.0       1.0       0.0       0.0         0.575758  \n",
       "  53           0.0       0.0       0.0       1.0         0.082078  \n",
       "  187          1.0       1.0       0.0       0.0         0.535065  \n",
       "  98           0.0       1.0       0.0       0.0         0.627706  \n",
       "  16           0.0       1.0       0.0       0.0         0.382597  \n",
       "  \n",
       "  [156 rows x 14 columns],\n",
       "  274    0.300532\n",
       "  146    0.505319\n",
       "  130    0.452128\n",
       "  341    0.385638\n",
       "  80     0.345745\n",
       "           ...   \n",
       "  213    0.106383\n",
       "  53     0.585106\n",
       "  187    0.226064\n",
       "  98     0.186170\n",
       "  16     0.239362\n",
       "  Name: mpg, Length: 156, dtype: float64),\n",
       " (     horsepower    weight  model_year_(69, 74]  model_year_(74, 79]  \\\n",
       "  141    0.201087  0.171817                  1.0                  0.0   \n",
       "  161    0.320652  0.647576                  0.0                  1.0   \n",
       "  114    0.239130  0.184860                  1.0                  0.0   \n",
       "  345    0.076087  0.041678                  0.0                  0.0   \n",
       "  221    0.538043  0.642756                  0.0                  1.0   \n",
       "  47     0.293478  0.473207                  1.0                  0.0   \n",
       "  263    0.646739  0.519422                  0.0                  1.0   \n",
       "  82     0.277174  0.253190                  1.0                  0.0   \n",
       "  39     0.701087  0.808336                  1.0                  0.0   \n",
       "  268    0.277174  0.194783                  0.0                  1.0   \n",
       "  185    0.179348  0.182024                  0.0                  1.0   \n",
       "  65     0.581522  0.713354                  1.0                  0.0   \n",
       "  8      0.972826  0.797278                  1.0                  0.0   \n",
       "  229    0.728261  0.739155                  0.0                  1.0   \n",
       "  261    0.402174  0.509498                  0.0                  1.0   \n",
       "  86     0.565217  0.583782                  1.0                  0.0   \n",
       "  29     0.228261  0.146583                  1.0                  0.0   \n",
       "  364    0.320652  0.598809                  0.0                  0.0   \n",
       "  298    0.429348  0.648426                  0.0                  1.0   \n",
       "  172    0.135870  0.172952                  0.0                  1.0   \n",
       "  291    0.429348  0.564786                  0.0                  1.0   \n",
       "  236    0.233696  0.323788                  0.0                  1.0   \n",
       "  325    0.010870  0.133825                  0.0                  0.0   \n",
       "  171    0.271739  0.308761                  0.0                  1.0   \n",
       "  160    0.347826  0.650411                  0.0                  1.0   \n",
       "  378    0.092391  0.145166                  0.0                  0.0   \n",
       "  133    0.293478  0.614687                  1.0                  0.0   \n",
       "  81     0.250000  0.191381                  1.0                  0.0   \n",
       "  115    0.538043  0.700028                  1.0                  0.0   \n",
       "  120    0.358696  0.355826                  1.0                  0.0   \n",
       "  252    0.320652  0.544939                  0.0                  1.0   \n",
       "  344    0.097826  0.074284                  0.0                  0.0   \n",
       "  371    0.206522  0.258577                  0.0                  0.0   \n",
       "  226    0.320652  0.513751                  0.0                  1.0   \n",
       "  95     0.972826  0.946413                  1.0                  0.0   \n",
       "  128    0.293478  0.488517                  1.0                  0.0   \n",
       "  155    0.141304  0.438049                  0.0                  1.0   \n",
       "  359    0.184783  0.458463                  0.0                  0.0   \n",
       "  27     0.891304  0.785086                  1.0                  0.0   \n",
       "  269    0.157609  0.174936                  0.0                  1.0   \n",
       "  \n",
       "       model_year_(79, 84]  cylinders_3  cylinders_4  cylinders_5  cylinders_6  \\\n",
       "  141                  0.0          0.0          1.0          0.0          0.0   \n",
       "  161                  0.0          0.0          0.0          0.0          1.0   \n",
       "  114                  0.0          0.0          1.0          0.0          0.0   \n",
       "  345                  1.0          0.0          1.0          0.0          0.0   \n",
       "  221                  0.0          0.0          0.0          0.0          0.0   \n",
       "  47                   0.0          0.0          0.0          0.0          1.0   \n",
       "  263                  0.0          0.0          0.0          0.0          1.0   \n",
       "  82                   0.0          0.0          1.0          0.0          0.0   \n",
       "  39                   0.0          0.0          0.0          0.0          0.0   \n",
       "  268                  0.0          0.0          1.0          0.0          0.0   \n",
       "  185                  0.0          0.0          1.0          0.0          0.0   \n",
       "  65                   0.0          0.0          0.0          0.0          0.0   \n",
       "  8                    0.0          0.0          0.0          0.0          0.0   \n",
       "  229                  0.0          0.0          0.0          0.0          0.0   \n",
       "  261                  0.0          0.0          0.0          0.0          1.0   \n",
       "  86                   0.0          0.0          0.0          0.0          0.0   \n",
       "  29                   0.0          0.0          1.0          0.0          0.0   \n",
       "  364                  1.0          0.0          0.0          0.0          0.0   \n",
       "  298                  0.0          0.0          0.0          0.0          0.0   \n",
       "  172                  0.0          0.0          1.0          0.0          0.0   \n",
       "  291                  0.0          0.0          0.0          0.0          0.0   \n",
       "  236                  0.0          0.0          1.0          0.0          0.0   \n",
       "  325                  1.0          0.0          1.0          0.0          0.0   \n",
       "  171                  0.0          0.0          1.0          0.0          0.0   \n",
       "  160                  0.0          0.0          0.0          0.0          1.0   \n",
       "  378                  1.0          0.0          1.0          0.0          0.0   \n",
       "  133                  0.0          0.0          0.0          0.0          1.0   \n",
       "  81                   0.0          0.0          1.0          0.0          0.0   \n",
       "  115                  0.0          0.0          0.0          0.0          0.0   \n",
       "  120                  0.0          0.0          1.0          0.0          0.0   \n",
       "  252                  0.0          0.0          0.0          0.0          1.0   \n",
       "  344                  1.0          0.0          1.0          0.0          0.0   \n",
       "  371                  1.0          0.0          1.0          0.0          0.0   \n",
       "  226                  0.0          0.0          0.0          0.0          1.0   \n",
       "  95                   0.0          0.0          0.0          0.0          0.0   \n",
       "  128                  0.0          0.0          0.0          0.0          1.0   \n",
       "  155                  0.0          0.0          0.0          0.0          1.0   \n",
       "  359                  1.0          0.0          1.0          0.0          0.0   \n",
       "  27                   0.0          0.0          0.0          0.0          0.0   \n",
       "  269                  0.0          0.0          1.0          0.0          0.0   \n",
       "  \n",
       "       cylinders_8  origin_1  origin_2  origin_3  crossed_feature  \n",
       "  141          0.0       0.0       1.0       0.0         0.128485  \n",
       "  161          0.0       1.0       0.0       0.0         0.649351  \n",
       "  114          0.0       0.0       1.0       0.0         0.111515  \n",
       "  345          0.0       0.0       0.0       1.0         0.074303  \n",
       "  221          1.0       1.0       0.0       0.0         0.508658  \n",
       "  47           0.0       1.0       0.0       0.0         0.497835  \n",
       "  263          0.0       1.0       0.0       0.0         0.384485  \n",
       "  82           0.0       0.0       0.0       1.0         0.149784  \n",
       "  39           1.0       1.0       0.0       0.0         0.645022  \n",
       "  268          0.0       0.0       0.0       1.0         0.151394  \n",
       "  185          0.0       1.0       0.0       0.0         0.148848  \n",
       "  65           1.0       1.0       0.0       0.0         0.638615  \n",
       "  8            1.0       1.0       0.0       0.0         0.636364  \n",
       "  229          1.0       1.0       0.0       0.0         0.617316  \n",
       "  261          0.0       1.0       0.0       0.0         0.523082  \n",
       "  86           1.0       1.0       0.0       0.0         0.453853  \n",
       "  29           0.0       0.0       0.0       1.0         0.092035  \n",
       "  364          1.0       1.0       0.0       0.0         1.000000  \n",
       "  298          1.0       1.0       0.0       0.0         0.903030  \n",
       "  172          0.0       0.0       1.0       0.0         0.105628  \n",
       "  291          1.0       1.0       0.0       0.0         0.541991  \n",
       "  236          0.0       1.0       0.0       0.0         0.231515  \n",
       "  325          0.0       0.0       1.0       0.0         0.186667  \n",
       "  171          0.0       0.0       0.0       1.0         0.161732  \n",
       "  160          0.0       1.0       0.0       0.0         0.688485  \n",
       "  378          0.0       1.0       0.0       0.0         0.115758  \n",
       "  133          0.0       1.0       0.0       0.0         0.584416  \n",
       "  81           0.0       0.0       0.0       1.0         0.134026  \n",
       "  115          1.0       1.0       0.0       0.0         0.636364  \n",
       "  120          0.0       0.0       1.0       0.0         0.173247  \n",
       "  252          0.0       1.0       0.0       0.0         0.616485  \n",
       "  344          0.0       1.0       0.0       0.0         0.092710  \n",
       "  371          0.0       1.0       0.0       0.0         0.222511  \n",
       "  226          0.0       1.0       0.0       0.0         0.524485  \n",
       "  95           1.0       1.0       0.0       0.0         0.715152  \n",
       "  128          0.0       1.0       0.0       0.0         0.584416  \n",
       "  155          0.0       1.0       0.0       0.0         0.692641  \n",
       "  359          0.0       0.0       1.0       0.0         0.346563  \n",
       "  27           1.0       1.0       0.0       0.0         0.591861  \n",
       "  269          0.0       1.0       0.0       0.0         0.112121  ,\n",
       "  141    0.531915\n",
       "  161    0.186170\n",
       "  114    0.452128\n",
       "  345    0.694149\n",
       "  221    0.226064\n",
       "  47     0.265957\n",
       "  263    0.231383\n",
       "  82     0.372340\n",
       "  39     0.132979\n",
       "  268    0.484043\n",
       "  185    0.452128\n",
       "  65     0.132979\n",
       "  8      0.132979\n",
       "  229    0.186170\n",
       "  261    0.242021\n",
       "  86     0.132979\n",
       "  29     0.478723\n",
       "  364    0.468085\n",
       "  298    0.372340\n",
       "  172    0.425532\n",
       "  291    0.271277\n",
       "  236    0.438830\n",
       "  325    0.938830\n",
       "  171    0.398936\n",
       "  160    0.212766\n",
       "  378    0.771277\n",
       "  133    0.186170\n",
       "  81     0.505319\n",
       "  115    0.159574\n",
       "  120    0.265957\n",
       "  252    0.271277\n",
       "  344    0.797872\n",
       "  371    0.531915\n",
       "  226    0.305851\n",
       "  95     0.079787\n",
       "  128    0.159574\n",
       "  155    0.159574\n",
       "  359    0.507979\n",
       "  27     0.053191\n",
       "  269    0.582447\n",
       "  Name: mpg, dtype: float64),\n",
       " (     horsepower    weight  model_year_(69, 74]  model_year_(74, 79]  \\\n",
       "  100    0.228261  0.399206                  1.0                  0.0   \n",
       "  281    0.211957  0.390417                  0.0                  1.0   \n",
       "  75     0.565217  0.698611                  1.0                  0.0   \n",
       "  204    0.130435  0.106890                  0.0                  1.0   \n",
       "  262    0.538043  0.513751                  0.0                  1.0   \n",
       "  ..          ...       ...                  ...                  ...   \n",
       "  174    0.277174  0.388716                  0.0                  1.0   \n",
       "  71     0.277174  0.203289                  1.0                  0.0   \n",
       "  357    0.293478  0.284094                  0.0                  0.0   \n",
       "  302    0.130435  0.152254                  0.0                  1.0   \n",
       "  173    0.277174  0.264247                  0.0                  1.0   \n",
       "  \n",
       "       model_year_(79, 84]  cylinders_3  cylinders_4  cylinders_5  cylinders_6  \\\n",
       "  100                  0.0          0.0          0.0          0.0          1.0   \n",
       "  281                  0.0          0.0          0.0          0.0          1.0   \n",
       "  75                   0.0          0.0          0.0          0.0          0.0   \n",
       "  204                  0.0          0.0          1.0          0.0          0.0   \n",
       "  262                  0.0          0.0          0.0          0.0          0.0   \n",
       "  ..                   ...          ...          ...          ...          ...   \n",
       "  174                  0.0          0.0          0.0          0.0          1.0   \n",
       "  71                   0.0          1.0          0.0          0.0          0.0   \n",
       "  357                  1.0          0.0          1.0          0.0          0.0   \n",
       "  302                  0.0          0.0          1.0          0.0          0.0   \n",
       "  173                  0.0          0.0          1.0          0.0          0.0   \n",
       "  \n",
       "       cylinders_8  origin_1  origin_2  origin_3  crossed_feature  \n",
       "  100          0.0       1.0       0.0       0.0         0.562771  \n",
       "  281          0.0       1.0       0.0       0.0         0.478788  \n",
       "  75           1.0       1.0       0.0       0.0         0.619394  \n",
       "  204          0.0       0.0       0.0       1.0         0.098701  \n",
       "  262          1.0       1.0       0.0       0.0         0.545628  \n",
       "  ..           ...       ...       ...       ...              ...  \n",
       "  174          0.0       1.0       0.0       0.0         0.277835  \n",
       "  71           0.0       0.0       0.0       1.0         0.012121  \n",
       "  357          0.0       0.0       0.0       1.0         0.153455  \n",
       "  302          0.0       1.0       0.0       0.0         0.119394  \n",
       "  173          0.0       0.0       0.0       1.0         0.198788  \n",
       "  \n",
       "  [196 rows x 14 columns],\n",
       "  100    0.239362\n",
       "  281    0.287234\n",
       "  75     0.132979\n",
       "  204    0.611702\n",
       "  262    0.271277\n",
       "           ...   \n",
       "  174    0.239362\n",
       "  71     0.265957\n",
       "  357    0.635638\n",
       "  302    0.678191\n",
       "  173    0.398936\n",
       "  Name: mpg, Length: 196, dtype: float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data module\n",
    "datamodule = DataModule()\n",
    "datamodule.load_data()\n",
    "feature_to_bin = \"model_year\"\n",
    "features_to_encode = [\"cylinders\", \"origin\"]\n",
    "features_to_cross = [\"displacement\", \"acceleration\"]\n",
    "intervals = pd.IntervalIndex.from_tuples([(69, 74), (74, 79), (79, 84)])\n",
    "\n",
    "\n",
    "datamodule.bin_feature(feature_to_bin, intervals)\n",
    "datamodule.one_hot_encode(features_to_encode)\n",
    "datamodule.cross_feature(features_to_cross[0], features_to_cross[1])\n",
    "datamodule.normalize()\n",
    "datamodule.train_val_test_split()\n",
    "#datamodule.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionTrainer:\n",
    "    def __init__(\n",
    "        self, num_features: int, learning_rate: float = 1e-3, num_epochs: int = 5000\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Inits the linear regression model.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_features = num_features\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.test_loss = None\n",
    "\n",
    "        self.theta = np.zeros(shape = self.num_features + 1)\n",
    "\n",
    "    def gradient_descent_step(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Perform a single step of gradient update on self.theta.\n",
    "\n",
    "        Args:\n",
    "            x: A matrix of features.\n",
    "            y: A vector of labels.\n",
    "        \"\"\"\n",
    "        self.theta = self.theta - (self.learning_rate/y.shape[0]) * self.mse_loss_derivative(x,y)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        x_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        x_val: np.ndarray,\n",
    "        y_val: np.ndarray,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run gradient descent for n epochs, where n = self.num_epochs.\n",
    "\n",
    "        Args:\n",
    "            x: A matrix of features.\n",
    "            y: A vector of labels.\n",
    "        \"\"\"\n",
    "        x_train = np.concatenate((np.ones((x_train.shape[0], 1)), x_train),axis = 1)\n",
    "        x_val = np.concatenate((np.ones((x_val.shape[0], 1)), x_val), axis = 1)\n",
    "        m = y_train.shape[0]\n",
    "        for i in range(self.num_epochs):\n",
    "            self.train_loss_history.append((np.sum((x_train @ self.theta - y_train)**2))/2/m)\n",
    "            self.val_loss_history.append((np.sum((x_val @ self.theta - y_val)**2))/2/m)\n",
    "            self.gradient_descent_step(x_train, y_train)\n",
    "    def mse_loss_derivative(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates the derivative of the loss function w.r.t. theta.\n",
    "\n",
    "        Args:\n",
    "            x: A matrix of features.\n",
    "            y: A vector of labels.\n",
    "\n",
    "        Returns:\n",
    "            A vector with the same dimension as theta, where each element is the\n",
    "            partial derivative of the loss function w.r.t. the corresponding element\n",
    "            in theta.\n",
    "        \"\"\"\n",
    "        return (x.T.dot((x @ self.theta - y)))\n",
    "\n",
    "    def evaluate(self, x_test: np.ndarray, y_test: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Evaluate the model on test set and store the test loss int self.test_loss.\n",
    "\n",
    "        Args:\n",
    "            x_test: A matrix of features.\n",
    "            y_test: A vector of labels.\n",
    "        \"\"\"\n",
    "        m = y_test.shape[0]\n",
    "        x_test = np.concatenate((np.ones((x_test.shape[0], 1)), x_test),axis = 1)\n",
    "        self.test_loss = (np.sum((x_test @ self.theta - y_test)**2))/2/m\n",
    "\n",
    "    def mse_loss(pred: np.ndarray, target: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the mean squared error given prediction and target. The equation is\n",
    "        given below.\n",
    "\n",
    "        mse = sum((pred - target) ^ 2) / (2 * n)\n",
    "\n",
    "        Args:\n",
    "            pred: A vector of predictions.\n",
    "            target: A vector of labels.\n",
    "\n",
    "        Returns:\n",
    "            Mean squared error between each element in pred and target.\n",
    "        \"\"\"\n",
    "        assert pred.shape == target.shape\n",
    "        m = pred.shape[0]\n",
    "        #print(m)\n",
    "        cost = (np.sum((X @ theta - y)**2))/2/m\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss: 0.026823521619587752\n",
      "Final validation loss: 0.005516032134163176\n",
      "[0.11327847 0.01857359 0.0237478  0.02529734 0.03925044 0.04873069\n",
      " 0.00190088 0.09001489 0.00348683 0.0164694  0.00140647 0.03476207\n",
      " 0.03306152 0.04545489 0.0224919 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.022913972513771263\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "iterations = 5000\n",
    "train, val, test = datamodule.train_val_test_split()\n",
    "\n",
    "X_train = train[0]\n",
    "y_train = train[1]\n",
    "trainer = LinearRegressionTrainer(X_train.shape[1] , learning_rate, iterations)\n",
    "\n",
    "X_val = val[0]\n",
    "y_val = val[1]\n",
    "\n",
    "trainer.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "print(f\"Final train loss: {trainer.train_loss_history[-1]}\")\n",
    "print(f\"Final validation loss: {trainer.val_loss_history[-1]}\")\n",
    "print(trainer.theta)\n",
    "\n",
    "plt.plot(np.arange(trainer.num_epochs), trainer.train_loss_history, label=\"Train loss\")\n",
    "plt.plot(np.arange(trainer.num_epochs), trainer.val_loss_history, label=\"Val loss\")\n",
    "plt.title(\"Train + validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "x_test = test[0]\n",
    "y_test = test[1]\n",
    "trainer.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {trainer.test_loss}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSE-144-Assignment-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
